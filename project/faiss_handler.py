import sys
import os
import faiss
import pickle
from sentence_transformers import SentenceTransformer

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config import Config

# üîπ Variables pour index et m√©tadonn√©es
INDEX_FILE = os.path.join(Config.INDEX_FAISS, "faiss_index.idx")
META_FILE = os.path.join(Config.INDEX_FAISS, "faiss_metadata.pkl")


def load_faiss_index():
    """
    Charge l'index FAISS et les m√©tadonn√©es si elles existent.
    """
    if os.path.exists(INDEX_FILE) and os.path.exists(META_FILE):
        print("üìÇ Index FAISS trouv√©, chargement...")
        index = faiss.read_index(INDEX_FILE)
        with open(META_FILE, "rb") as f:
            data = pickle.load(f)
        chunks = data["chunks"]
        metadata = data["metadata"]
        embedder = SentenceTransformer(Config.RAG_MODEL)
        return chunks, metadata, embedder, index
    else:
        print("‚ö†Ô∏è Aucun index existant.")
        return None, None, None, None


def save_faiss_index(index, chunks, metadata):
    """
    Sauvegarde l'index FAISS et les m√©tadonn√©es.
    """
    os.makedirs(os.path.dirname(INDEX_FILE), exist_ok=True)
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "wb") as f:
        pickle.dump({"chunks": chunks, "metadata": metadata}, f)

    doc_names = [m.get("path", "inconnu") for m in metadata]
    print("‚úÖ Index FAISS et m√©tadonn√©es sauvegard√©s pour les documents :", ", ".join(doc_names))
    print("--------------------------")


def build_faiss_index(chunks, metadata):
    """
    Cr√©e un nouvel index FAISS √† partir de chunks et metadata.
    """
    if not chunks:
        print("‚ö†Ô∏è Aucun chunk disponible")
        return None, None

    embedder = SentenceTransformer(Config.RAG_MODEL)
    embeddings = embedder.encode(chunks, convert_to_numpy=True)
    print("Shape des embeddings :", embeddings.shape)

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)

    save_faiss_index(index, chunks, metadata)
    print(f"Index FAISS cr√©√© avec {index.ntotal} vecteurs")
    return embedder, index


def faiss_index_handler(new_chunks, new_metadata):
    """
    Ajoute de nouveaux chunks √† l'index existant ou cr√©e un nouvel index si n√©cessaire.
    """
    chunks, metadata, embedder, index = load_faiss_index()

    # Si aucun index existant, on le cr√©e directement
    if chunks is None or metadata is None or index is None:
        print("‚ö†Ô∏è Cr√©ation d'un nouvel index FAISS pour les nouveaux documents...")
        embedder, index = build_faiss_index(new_chunks, new_metadata)
        return new_chunks, new_metadata, embedder, index  # <-- Retourne bien 4 valeurs

    # D√©tecter les nouveaux documents pour √©viter les doublons
    new_entries = []
    existing_filenames = [m.get("path") for m in metadata]
    for chunk, meta in zip(new_chunks, new_metadata):
        if meta.get("path") not in existing_filenames:
            new_entries.append((chunk, meta))

    if not new_entries:
        print("‚ÑπÔ∏è Aucun nouveau document √† ajouter.")
        return chunks, metadata, embedder, index

    # Ajouter les embeddings des nouveaux chunks
    chunks_to_add, metadata_to_add = zip(*new_entries)
    embeddings_to_add = embedder.encode(list(chunks_to_add), convert_to_numpy=True)
    index.add(embeddings_to_add)

    # Mettre √† jour les listes
    chunks.extend(chunks_to_add)
    metadata.extend(metadata_to_add)

    save_faiss_index(index, chunks, metadata)
    print(f"‚úÖ {len(chunks_to_add)} nouveaux documents ajout√©s √† l'index FAISS.")

    return chunks, metadata, embedder, index


def retrieve(query, embedder, index, chunks, metadata, top_k=5):
    """
    Recherche les top_k chunks les plus proches d'une query.
    Version s√©curis√©e pour √©viter les erreurs.
    """
    if index is None or index.ntotal == 0:
        print("‚ö†Ô∏è L'index FAISS est vide.")
        return []

    if not chunks or not metadata:
        print("‚ö†Ô∏è Les chunks ou metadata sont vides.")
        return []

    # Assurer que top_k ne d√©passe pas le nombre de vecteurs
    top_k = min(top_k, index.ntotal)

    query_vector = embedder.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vector, top_k)

    results = []
    for idx in indices[0]:
        if idx < len(chunks):
            results.append({
                "text": chunks[idx],
                "metadata": metadata[idx]
            })
        else:
            print(f"‚ö†Ô∏è Index FAISS {idx} hors limites pour chunks (len={len(chunks)})")

    return results
