import sys
import os
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from config import Config
import math

# üîπ Variables pour index et m√©tadonn√©es
INDEX_FILE = os.path.join(Config.INDEX_FAISS, "faiss_index.idx")
META_FILE = os.path.join(Config.INDEX_FAISS, "faiss_metadata.pkl")


def load_faiss_index():
    if os.path.exists(INDEX_FILE) and os.path.exists(META_FILE):
        print("üìÇ Index FAISS trouv√©, chargement...")
        index = faiss.read_index(INDEX_FILE)
        with open(META_FILE, "rb") as f:
            data = pickle.load(f)
        chunks = data["chunks"]
        metadata = data["metadata"]
        embedder = SentenceTransformer(Config.RAG_MODEL)
        print("Load count chunks:", len(chunks))
        return chunks, metadata, embedder, index
    else:
        print("‚ö†Ô∏è Aucun index existant.")
        return None, None, None, None


def save_faiss_index(index, chunks, metadata):
    """
    Sauvegarde l'index FAISS et les m√©tadonn√©es.
    """
    os.makedirs(os.path.dirname(INDEX_FILE), exist_ok=True)
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "wb") as f:
        pickle.dump({"chunks": chunks, "metadata": metadata}, f)

    doc_names = [m.get("path", "inconnu") for m in metadata]
    print("‚úÖ Index FAISS et m√©tadonn√©es sauvegard√©s pour les documents :", ", ".join(doc_names))
    print("--------------------------")


def build_faiss_index(chunks, metadata):
    if not chunks:
        print("‚ö†Ô∏è Aucun chunk √† indexer")
        return None, None

    embedder = SentenceTransformer(Config.RAG_MODEL)
    embeddings = embedder.encode(chunks, convert_to_numpy=True)
    embeddings = normalize(embeddings, axis=1)  # ||v|| = 1

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)  # inner product = cosine similarity
    index.add(embeddings)

    save_faiss_index(index, chunks, metadata)
    print(f"‚úÖ Index FAISS cr√©√© avec {index.ntotal} vecteurs normalis√©s")
    return embedder, index


def faiss_index_handler(new_chunks, new_metadata):
    """
    Ajoute de nouveaux chunks √† l'index existant ou cr√©e un nouvel index si n√©cessaire.
    """
    chunks, metadata, embedder, index = load_faiss_index()

    # Si aucun index existant, on le cr√©e directement
    if chunks is None or metadata is None or index is None:
        print("‚ö†Ô∏è Cr√©ation d'un nouvel index FAISS pour les nouveaux documents...")
        embedder, index = build_faiss_index(new_chunks, new_metadata)
        return new_chunks, new_metadata, embedder, index  # <-- Retourne bien 4 valeurs

    # D√©tecter les nouveaux documents pour √©viter les doublons
    new_entries = []
    existing_filenames = [m.get("path") for m in metadata]
    for chunk, meta in zip(new_chunks, new_metadata):
        if meta.get("path") not in existing_filenames:
            new_entries.append((chunk, meta))

    if not new_entries:
        print("‚ÑπÔ∏è Aucun nouveau document √† ajouter.")
        return chunks, metadata, embedder, index

    # Ajouter les embeddings des nouveaux chunks
    chunks_to_add, metadata_to_add = zip(*new_entries)
    embeddings_to_add = embedder.encode(list(chunks_to_add), convert_to_numpy=True)
    index.add(embeddings_to_add)

    # Mettre √† jour les listes
    chunks.extend(chunks_to_add)
    metadata.extend(metadata_to_add)

    save_faiss_index(index, chunks, metadata)
    for f in  list({m.get("path") for m in metadata_to_add}):
      print(f"Ajout du Document  : {f}")
      
    print(f"‚úÖ {len(chunks_to_add)} nouveaux documents ajout√©s √† l'index FAISS.")
    return chunks, metadata, embedder, index


import numpy as np

def retrieve(query, top_k=5, min_score=Config.RAG_MIN_SCORE): # 0.5 tolerance 0.7 strict
    chunks, metadata, embedder, index = load_faiss_index()
    if not embedder or index is None or not chunks or not metadata:
        return []

    top_k = min(top_k, index.ntotal)

    # Embedding de la query et normalisation
    query_vector = embedder.encode([query], convert_to_numpy=True)
    query_vector = normalize(query_vector, axis=1).astype("float32")

    # Recherche FAISS
    scores, indices = index.search(query_vector, top_k)

    results = []

    for dist, idx in zip(scores[0], indices[0]):
        if idx >= len(chunks):
            continue
       
        alpha = 1.0
        score = math.exp(-alpha * dist) # lissage exp 0-1
        
        print("score:", score, "doc:", metadata[idx]["source"])
        if score >= min_score:  #  plus le score est proche de 1 plus la prediction est bonne ! 
            results.append({
                "text": chunks[idx],
                "metadata": metadata[idx],
                "score": float(score)
            })

    return results
