import io
import base64
from PIL import Image
from concurrent.futures import ThreadPoolExecutor, TimeoutError

from config import Config
from ia.faiss.faiss_handler import retrieve
from ia.history_handler import filter_relevant_history, add_user_query
from ia.web_search_handler import searchWeb

from ia.generate_repport import repportLLM
from ia.generate_repport_trs import  repportLLM_TRS
from config import Config
from ia.faiss.faiss_handler import retrieve

# =========================================================
# CHAT TEMPLATE GGUF
# =========================================================
def build_chat_prompt(system, user):
    return f"""<|system|>
{system}
<|end|>

<|user|>
{user}
<|end|>

<|assistant|>
"""


# =========================================================
# GGUF GENERATION
# =========================================================
def run_gguf_generation(model, prompt):
    result = model(
        prompt,
        max_tokens=Config.MAX_OUTPUT_TOKEN,
        temperature=Config.TEMPERATURE,
        top_p=Config.TOP_P,
        top_k=Config.TOP_K,
        repeat_penalty=1.08,
        stop=["<|end|>"]
    )
    return result["choices"][0]["text"].strip()


# =========================================================
# FAISS + RAG
# =========================================================
def faiss_search(user_ip, query, model, workeflow):
    retrieved = retrieve(user_ip="none",query = query, workeflow = workeflow)

    if not retrieved:
        print("‚ö†Ô∏è Aucun chunk pertinent (score < threshold). Fallback vers LLM brut.")
        return prompt_query(user_ip, query, model)

    context = "\n\n".join([
        f"Texte: {r['text']}...\n"
        f"Chemin: {r['metadata'].get('path','inconnu')}\n"
        f"Source: {r['metadata'].get('source','inconnu')}\n"
        f"Page: {r['metadata'].get('page','?')}\n"
        f"Score: {round(r.get('score', 0), 3)}"
        for r in retrieved
    ])

    system_prompt = """Tu es un assistant fran√ßais RAG.
        Tu dois r√©pondre UNIQUEMENT √† partir du contexte fourni.
        Aucune information ext√©rieure ne doit √™tre ajout√©e.
        R√©ponse factuelle, concise, directe.
        """

    user_prompt = f"""=== Contexte ===
{context}

=== Question ===
{query}

R√®gles :
- Ne r√©p√®te pas le contexte
- Ne reformule pas la question
- Pas de Markdown
- Pas d‚Äôexplication
"""

    prompt = build_chat_prompt(system_prompt, user_prompt)

    with ThreadPoolExecutor() as executor:
        future = executor.submit(run_gguf_generation, model, prompt)
        try:
            return future.result(timeout=Config.SERVER_TIMEOUT)
        except TimeoutError:
            return f"‚è±Ô∏è La g√©n√©ration a d√©pass√© le d√©lai imparti ({Config.SERVER_TIMEOUT} sec)"


# =========================================================
# PROMPT DIRECT (HISTORIQUE + WEB)
# =========================================================
def prompt_query(user_ip, query, model):
    history = filter_relevant_history(user_ip, query)
    add_user_query(user_ip, query)

    history_text = ""
    if history:
        history_text = "\n".join([f"- {h}" for h in history])

    web_results = searchWeb(query)
    web_text = ""
    if web_results:
        web_text = "\n".join([
            f"{r['title']} | {r['url']}\n{r.get('snippet','')}"
            for r in web_results
        ])

    system_prompt = """Tu es un assistant fran√ßais.
Tu dois r√©pondre UNIQUEMENT √† partir des √©l√©ments fournis.
R√©ponse courte, pr√©cise, directe.
"""

    user_prompt = f"""
Historique pertinent :
{history_text if history_text else "Aucun"}

Contexte web :
{web_text if web_text else "Aucun"}

Question :
{query}

R√®gles strictes :
- Ne pose pas de questions
- Ne r√©p√®te pas la question
- Ne mentionne pas l‚Äôhistorique
- Ne commente rien
- Pas de Markdown
"""

    prompt = build_chat_prompt(system_prompt, user_prompt)

    with ThreadPoolExecutor() as executor:
        future = executor.submit(run_gguf_generation, model, prompt)
        try:
            return future.result(timeout=Config.SERVER_TIMEOUT)
        except TimeoutError:
            return f"‚è±Ô∏è La g√©n√©ration a d√©pass√© le d√©lai imparti ({Config.SERVER_TIMEOUT} sec)"


# =========================================================
# EVAL PROMPT SIMPLE
# =========================================================
def eval_prompt(prompt, model):
    system_prompt = "Tu es un assistant fran√ßais."
    final_prompt = build_chat_prompt(system_prompt, prompt)

    with ThreadPoolExecutor() as executor:
        future = executor.submit(run_gguf_generation, model, final_prompt)
        try:
            return future.result(timeout=2000)
        except TimeoutError:
            return f"‚è±Ô∏è La g√©n√©ration a d√©pass√© le d√©lai imparti (2000 sec)"


def eval_prompt_anomaly_gguf(
    system_prompt: str,
    user_prompt: str,
    model,
    anomalie: dict
):
    # ============================================================
    # RAG
    # ============================================================
    query = (
        f"Machine {anomalie['machine']} "
        f" {anomalie['stepId']} "
        f" {anomalie['stepName']} "
    )
    
    print("RAG QUERY : " + query)

    retrieved = retrieve(
        user_ip="none",
        query=query,
        workflow=True
    )

    if retrieved:
        rag_block = (
            "DOCUMENTATION TECHNIQUE DISPONIBLE (USAGE STRICTEMENT FACTUEL)\n"
            "Les extraits suivants peuvent √™tre utilis√©s UNIQUEMENT s‚Äôils "
            "sont directement applicables aux r√®gles d√©clench√©es.\n\n"
            + "\n\n".join([
                f"- Extrait :\n{r['text']}\n"
                f"Source : {r['metadata'].get('source','?')} | "
                f"Page : {r['metadata'].get('page','?')} | "
                f"Score : {round(r.get('score', 0), 3)}"
                for r in retrieved
            ])
        )
    else:
        rag_block = (
            "Aucune documentation technique pertinente disponible.\n"
            "L‚Äôanalyse DOIT √™tre bas√©e exclusivement sur les r√®gles d√©clench√©es "
            "et les observations factuelles."
        )

    # ============================================================
    # PROMPT FINAL UTILISATEUR
    # ============================================================
    final_prompt = f"""
{system_prompt}

{user_prompt}

===========================
CONTEXTE DOCUMENTAIRE
===========================
{rag_block}

===========================
R√àGLES ABSOLUES D‚ÄôANALYSE
===========================
- Analyse STRICTEMENT factuelle
- Aucune hypoth√®se non d√©duite des donn√©es
- Si erreur PLC explicite : analyse √©v√©nementielle prioritaire
- Les dur√©es et d√©phasages sont des CONS√âQUENCES, jamais des causes
- Si donn√©es insuffisantes : le dire explicitement

===========================
FORMAT DE SORTIE STRICT
===========================
- Rapport structur√©
- Phrases courtes
- Chiffres syst√©matiques
- Aucun commentaire hors donn√©es
"""

    # ============================================================
    # APPEL LLM ‚Äî GGUF VERROUILL√â
    # ============================================================
    print ("[RUN LLM] Wait Anomaly Result ")
    print(final_prompt)
    output = model.create_chat_completion(
        messages=[
            {
                "role": "system",
                "content": (
                    """
                    LANGUE DE SORTIE OBLIGATOIRE : FRAN√áAIS UNIQUEMENT.

                    INTERDICTION ABSOLUE :
                    - Toute utilisation de mots, phrases ou expressions en anglais.
                    - Toute sortie partiellement ou totalement en anglais est STRICTEMENT INTERDITE
                    et sera consid√©r√©e comme INVALIDE.

                    R√àGLE DE CONFORMIT√â :
                    - La r√©ponse doit √™tre int√©gralement r√©dig√©e en fran√ßais.
                    - Les termes techniques doivent √™tre traduits ou explicit√©s en fran√ßais.
                    - Aucun anglicisme, acronyme ou terme non traduit n‚Äôest autoris√©.

                    UTILISATION DE DOCUMENTATION TECHNIQUE :
                    - Si la r√©ponse s‚Äôappuie sur une documentation technique disponible,
                    tu DOIS obligatoirement le pr√©ciser explicitement sous la forme suivante :

                    ¬´ Selon la documentation technique de r√©f√©rence : [nom du document] ¬ª

                    Toute r√©ponse ne respectant pas strictement ces r√®gles est consid√©r√©e comme NON CONFORME.

                """
                 
                )
            },
            {
                "role": "user",
                "content": final_prompt
            }
        ],
        temperature=0.1,   # üîí factuel
        top_p=0.8,
        top_k=40,
        repeat_penalty=1.1,
        max_tokens=2500
    )

    result = output["choices"][0]["message"]["content"].strip()

    if not result or len(result) < 50:
        raise RuntimeError("LLM output invalide ou vide (Anomaly)")

    print ("[RUN LLM] SUCCESS Anomaly Result length => ",  len(result) )
 
    return repportLLM(result, anomalie, final_prompt)

########## TRS ############

def eval_prompt_trs_gguf(
    prompt: str,
    trs: dict,
    impact: list,
    dateStart,
    dateEnd,
    model
):

    # ============================================================
    # CONTEXTE STRUCTUR√â POUR LE LLM (DONN√âES)
    # ============================================================
    trs_block = f"""
TRS GLOBAL :
- TRS            : {trs['trs']}
- Performance    : {trs['performance']}
- Qualit√©        : {trs['quality']}
- Steps analys√©s : {trs['totalSteps']}
- Steps NOK      : {trs['badSteps']}
- Temps nominal  : {trs['totalTheoreticalTimeS']} s
- Temps r√©el     : {trs['totalRealTimeS']} s
"""

    if impact:
        impact_block = "\n".join([
            f"- Machine={i['machineCode']} | Step={i['stepCode']} | "
            f"Occ={i['occurrences']} | Overrun={i['totalOverrunS']} s | "
            f"ImpactTRS={i['impactPercentTRS']} % | "
            f"Danger={i['dangerScore']} | "
            f"Renforcement={i['reinforcing']}"
            for i in impact.values()
        ])
    else:
        impact_block = (
            "Aucune anomalie mesur√©e disponible.\n"
            "Toute conclusion doit refl√©ter explicitement cette absence de donn√©es."
        )

    # ============================================================
    # PROMPT FINAL (UTILISATEUR)
    # ============================================================
    final_prompt = f"""
{prompt}

===========================
P√âRIODE ANALYS√âE
===========================
{dateStart} ‚Üí {dateEnd}

===========================
DONN√âES TRS MESUR√âES
===========================
{trs_block}

===========================
IMPACTS PAR STEP / MACHINE
===========================
{impact_block}

===========================
R√àGLES D‚ÄôANALYSE STRICTES
===========================
- Analyse UNIQUEMENT bas√©e sur les donn√©es fournies
- Aucune hypoth√®se non d√©duite des chiffres
- Aucun ajout externe
- Aucun conseil ou recommandation
- Comparaison STRICTE r√©el vs nominal
- Tous les impacts DOIVENT √™tre chiffr√©s (temps et %)
- Identifier UNIQUEMENT les causes MAJEURES
- Si donn√©es insuffisantes : le dire explicitement

===========================
FORMAT DE SORTIE OBLIGATOIRE
===========================
ANOMALIES MAJEURES :
1. <description> ‚Äî <temps>s ‚Äî <impact %>
2. <description> ‚Äî <temps>s ‚Äî <impact %>
3. <description> ‚Äî <temps>s ‚Äî <impact %>

CONTRIBUTION CUMUL√âE :
- <valeur %>

NATURE DE LA D√âGRADATION :
- STRUCTURELLE ou PONCTUELLE

CONCLUSION FACTUELLE :
- Ligne 1
- Ligne 2
- Ligne 3
"""

    # ============================================================
    # APPEL LLM (GGUF VERROUILL√â)
    # ============================================================
    print ("[RUN LLM] Wait TRS Result ")

    output = model.create_chat_completion(
        messages=[
            {
                "role": "system",
                "content": (
                    """
                    LANGUE DE SORTIE OBLIGATOIRE : FRAN√áAIS UNIQUEMENT.
                    INTERDICTION ABSOLUE :
                    - anglais
                    TOUTE SORTIE CONTENANT DE L‚ÄôANGLAIS EST CONSID√âR√âE COMME INVALIDE.
                """
                )
            },
            {
                "role": "user",
                "content": final_prompt
            }
        ],
        temperature=0.1,   # üîí TR√àS factuel
        top_p=0.8,
        top_k=40,
        repeat_penalty=1.1,
        max_tokens=2500
    )

    result = output["choices"][0]["message"]["content"].strip()

    if not result or len(result) < 50:
        raise RuntimeError("LLM output invalide ou vide (TRS)")

    print ("[RUN LLM] SUCCESS TRS Result length => ",  len(result) )

    # ============================================================
    # RAPPORT (INCHANG√â)
    # ============================================================
    return repportLLM_TRS(
        result,
        trs,
        impact,
        dateStart,
        dateEnd,
        final_prompt
    )
